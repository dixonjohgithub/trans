# Why Andrej Karpathy Feels "Behind" (And What It Means for Your Career)

---

## Video Information

| Property | Value |
|----------|-------|
| **URL** | [https://www.youtube.com/watch?v=fyHnGHxGuhI](https://www.youtube.com/watch?v=fyHnGHxGuhI) |
| **Channel** | AI News & Strategy Daily | Nate B Jones |
| **Duration** | 25:09 |
| **Processed** | 2026-01-07 08:26 |

---

## TL;DR

> The video argues that AI has triggered a phase shift from deterministic coding to orchestrating probabilistic systems, requiring everyone‚Äînot just engineers‚Äîto learn a new skill tree that separates generation from decision-making to preserve authority and achieve scalable leverage.

---

## Key Points

### 1. Karpathy‚Äôs ‚Äúbehind‚Äù feeling signals a real job refactor, not individual failure

The speaker uses Andrej Karpathy‚Äôs admission that he feels unusually behind to illustrate a broader workforce reality: tools and capabilities change so fast that mental models decay within weeks. This creates emotional whiplash because traditional markers of competence (control, mastery, authorship) no longer align with how work gets done. Leaders should treat this as a systemic transition rather than a personal skills gap.

### 2. Technical leverage shifted from writing deterministic code to orchestrating probabilistic machines

Historically, leverage came from writing correct instructions (software) that executed deterministically and could be inspected, debugged, and patched. With LLMs, the core component is stochastic and partially opaque, producing plausible outputs that cannot be single-stepped or fully explained. As a result, ‚Äúbeing technical‚Äù increasingly means designing systems that can harness probabilistic generation reliably.

### 3. Four foundational breaks: control, effort-to-output, abstraction direction, and role boundaries

First, control is no longer the default because you steer model behavior rather than author exact outcomes. Second, effort no longer maps cleanly to output because delegation loops can yield disproportionate leverage compared to manual execution. Third, the abstraction stack inverts: work starts from intent, jumps to generated artifacts, and then relies on verification rather than incremental construction. Fourth, the key divide shifts from engineer vs. non-engineer to people who can delegate generation while retaining authority vs. those who cannot.

### 4. Root principle: separate generation from decisioning to preserve authority

The video frames the central rule for reliable AI work as keeping LLMs in the role of fast generators, not final judges of truth, safety, or correctness. The workflow (humans and deterministic checks) must own decisions about what ships, what is approved, and what is correct. Most workplace failures occur when organizations let a token generator implicitly become the authority.

### 5. Level 1‚ÄîConditioning: intent specification, context engineering, and constraint design

Conditioning reduces variance by making inputs precise and operational. It includes tight problem contracts (purpose, audience, constraints), deliberate management of what enters the context window (what to quote, summarize, or exclude), and strong constraints (schemas, rubrics, citations, tool allowances, budgets, stop conditions). Without these, probabilistic systems behave like slot machines; with them, they become steerable components.

### 6. Level 2‚ÄîAuthority: verification, provenance, and permissions

Authority replaces the old guarantee that came from authored logic by adding explicit checks and accountability mechanisms. Verification can be deterministic (schema checks, unit tests) or procedural (human review, critique passes, adversarial testing), but it must be designed in. Provenance provides chain-of-custody through sources, citations, retrieved documents, and audit-ready traceability. Permissions must remain deterministic and least-privilege; the model cannot be the security boundary, especially as agent capabilities expand.

### 7. Level 3 & 4‚ÄîWorkflows and Compounding: pipelines, observability, evals, feedback loops, drift governance

Workflows scale LLM use by decomposing tasks into pipeline steps with checkpoints so failures stay local and systems become reusable by others. Teams need failure-mode taxonomies (context missing, retrieval wrong, tool errors, constraint conflicts, hallucinations, refusals, budget overruns) and strong observability (tool-call traces, inputs, retrieved docs, intermediate outputs, validations, timing, cost). Compounding requires evaluation harnesses to prevent ‚ÄúRussian roulette‚Äù when changing prompts/models/tools, plus feedback loops (draft‚Üícritique‚Üírevise‚Üíverify‚Üíship) and drift management/governance (versioning, policies, auditability) to keep control under continuous change.

### 8. The Factorio analogy: durable advantage comes from building automated factories, not handcrafting outputs

The speaker compares the new skill ladder to Factorio, where progress comes from moving from manual crafting to automated, observable, modular production systems. The metaphor reinforces that modern competence lies in designing systems that reliably produce outcomes at scale. Organizations should reward workflow design and governance over individual authorship, because scalable reliability matters more than who manually produced each artifact.

---

## Conclusion

The video claims AI has created a phase transition in leverage: success now depends less on deterministic authorship and more on orchestrating probabilistic systems without surrendering authority. It proposes a workforce-wide skill tree rooted in separating generation from decision-making, then progressing through conditioning, authority, workflows, and compounding with evals and governance. The core career takeaway is that feeling ‚Äúbehind‚Äù reflects a changing stack, not personal inadequacy. Organizations that deliberately train these skills across roles can unlock major productivity gains, while those clinging to old technical/non-technical hierarchies risk falling behind.

---

## References & Mentions

### üë§ People & Authors

- Andrej Karpathy - cited for saying he has never felt this behind as a programmer and for discussing the shift in technical leverage
- Bill Gates - used as an example of leverage in the deterministic software era (Windows)

### üîß Websites & Tools

- natebjones.com - https://natebjones.com
- Nate's Newsletter (Full Story link) - https://natesnewsletter.substack.com/p/executive-briefing-the-leveling-crisis?r=1z4sm5&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true
- Opus 4.5 - referenced as an example model/tool affecting world models quickly (no URL provided)

### üìå Other References

- Windows 95 - referenced as a deterministic-software-era example
- Factorio - referenced as an analogy for building automated, scalable systems

---

## Topics

`AI leadership` ‚Ä¢ `LLM workflow design` ‚Ä¢ `probabilistic systems` ‚Ä¢ `delegation and authority` ‚Ä¢ `verification and governance` ‚Ä¢ `prompting and context engineering` ‚Ä¢ `evaluation harnesses` ‚Ä¢ `future of technical skills`

---

## Raw Transcript

<details>
<summary>Click to expand full transcript</summary>

```
If Andrej Karpathy says he's never felt this behind as a programmer, which he did, all of us should be glad to know as much as we have and should not feel bad about trying to learn more. We're all in the same boat together. What has happened over the last year is a phased transition in technical leverage. And that's what Andrej spent a lot of his time talking about over the holiday week between Christmas and New Year's. And that's what I want to talk about here in my executive briefing today. Fundamentally, what changed is what it means to be technical. And the new technical skill tree that's getting unlocked is no longer just for engineers. As a leader, you need to think about anyone in your organization who needs the authority to tell probabilistic machines, namely large language models, how they can usefully generate work. And so today I'm going to take Andrej's reflection seriously and reflect back on it and try to lay out a useful skill tree that talks about what's changing, why these new skills feel hard, and how as org leaders we can start to lay out skill levels and trees that feel useful, not just for engineers, but for everyone in the business. Let's start by saying the obvious thing that most people tend to dance around. This is hard. Andrej feels behind. It's because the job is genuinely being refactored while we are all working as quickly as we can. Tools are changing every week. Capabilities keep jumping. Mental models decay quickly. One of the things Andrej was observing that I think is really true is that if you haven't played with Opus 4.5 from a technical perspective in the last month, your world model is already outdated. And that's just four weeks ago. And the emotional whiplash isn't just it's a new tool. It's that the old way that we anchored on our sense of competence, what it meant to be skilled, what it means to have control over our tools and our craft, all of that has to change because it stopped matching reality. For most of modern engineering history, leverage came from writing more correct instructions faster than other people. Sure. But really, more correct instructions on problems that mattered. So if you really wanted leverage in the engineering discipline, you picked the problems that mattered. You wrote correct instructions, i.e. workflows, i.e. programs. And then you were able to leverage those at a wide scale. In a very simplified fashion, that is the story of Bill Gates and Windows. You internalize abstractions. You master your tools. You shape deterministic systems. And before you know it, you have Windows 95. You write logic. The machine executes the logic the exact same way on every single CD copy that Windows hands out. When something breaks, the system can be inspected. You can ship a bug patch. You can trace causality. You can step through the behavior end to end and find out what's wrong in that old world. Authorship and authority were really tightly linked. If you were the person who wrote the program, you had the authority to fix it and also the knowledge to fix it. You wrote the behavior. So you owned the behavior. The assumption of control is baked into the very rituals we have as software engineers where we talk about the idea of the engineer who knows the code, the engineer who authors the code, the engineer who knows where all of the skeletons are in the code base because that engineer touched it, because that engineer knows it. This is why the conventional wisdom for the last several decades has been that no matter how frustrated you are with your founding engineer, you keep them around because they know the code. That whole system of assumptions is changing. That regime is ending. The unit of leverage is shifting from writing code toward orchestrating intelligence. And that's not just a buzzword. Intelligence here doesn't mean a magical AGI vibe. It means a very specific kind of component that has entered the stack that's net new. It's a probabilistic component. It's stochastic. It's fallible. It's changing. You can pick your adjectives, but you see the idea. The model is not a deterministic function. An LLM is a probabilistic token generator. It produces a plausible sequence conditioned on inputs, and its internal reasoning is not something you can fully inspect the way that you inspect your code. You can't single step it through. You can't rubber duck it if you're an engineer. You can't reliably reproduce it. You can't treat it like a clean abstraction. It's truly an alien component that has really weird ergonomics for traditional software engineers. And that's why this moment feels like an earthquake. We didn't just get a better Python library, right? We got a new kind of machine in the loop. And once you accept that, you can explain almost everything that people are feeling, especially the best engineers, because the assumptions that engineers trained on are breaking in a number of specific ways. And if you're wondering as a leader why this matters for you, I got news. Your whole technical team is wrestling with this, and now it's no longer a technical team issue. You're going to have to expand the blast radius and understand how the skill trees we talk about in this video touch all of your job families, not just the technical. But first, let's understand what has broken so that we can rebuild on top of it correctly. The first thing that broke is that control is not the default anymore. In the old world, when you authored behavior, it was yours. And in the new world, you condition behavior, right? You don't author it specifically. You can shape outcomes through prompts, through context windows, through memory structures or tool access, and the model responds probabilistically. The same input can yield somewhat different outputs. The same workflow, it is possible it can drift when the underlying model changes. In fact, it's likely. Mastery is then less about, I can make it do exactly what I want every time the same way, and more about, I can steer it toward X outcome reliably, I can detect when it's off, and I can correct it very quickly. The mental shift is from authorship to steering. Second thing that breaks, effort no longer clearly maps to output. In a deterministic world, being better meant you could, frankly, do more with your time, right? You were faster at typing, faster at debugging, better recall, better architecture. As long as you worked on the right problem, you were going to have more leverage. In a probabilistic world, that bottleneck moves. Sometimes one person gets a 10x jump because they know how to set up a delegation loop, while another person grinds away manually and gets less done despite being just as smart. And that's what Andre is calling a skill issue these days. The skill is new. It's unintuitive. It's hierarchical. It's the need to develop a skill of delegation instead of a skill of execution. And failure to learn it means your effort just doesn't convert into leverage in the new AI economy. The third thing that broke is that the abstraction stack got inverted. Historically, high-level reasoning collapsed downward into code very cleanly, right? You have your intention, and it collapses into implementation. This is where the whole idea of product management and requirements comes from. But now, low-level implementation often expands upward from intent. You end up in a place where you have intent, and you jump straight to generated artifacts, and then you verify the output. In other words, the job shifts away from constructing something toward supervising a construction crew. You have a defined goal. You've defined constraints for your workspace. You have evaluations the system needs to pass, and you have correction methodologies. And now the work moves from write an instruction to can you design that system so the system self-evolves until it hits the correct behavior. So in that world, the intention shifts into, okay, I have a desire to make a particular piece of program that can do a particular task for me. Here are my evals. Here are my constraints. I'm going to let it generate artifacts, maybe code bases or pull requests, until it passes my evals, and then I have a verified output I can actually do something with. That's a whole new way of doing things. It changes the way we think about abstraction. Fourth, the old boundaries of engineering don't make sense anymore. The most important divide used to be between engineer and non-engineer, and now it's between someone who can delegate and someone who can't. And that phrase of like the concept of preserving authority while delegating generation is a core of the new skills we need in the AI world, and it's not limited to engineering. If you think about what we are doing with AI, we still have to be in charge. We still have to preserve the authority, but we do have to delegate to get leverage. Authority used to come for free for engineers when they wrote the code, because if I write the code, I can justify the behavior of the system. I can point to this line and I can explain the root cause. But in a probabilistic world, the machine will generate behavior, and you lose that natural chain of custody. You're not automatically in authority on what the machine is doing. It is possible to ship something correct without fully understanding why. And you can also ship something wrong that looks correct. So the center of the craft is changing to how do you design a workflow where you can delegate a huge amount of the generation activity, but ensure human authority over what is actually shipping. That is the new technical skill tree, but it turns out that's not just for technical people. I don't know that I believe in technical people anymore. It's for everybody, because every profession is becoming some version of orchestrate probabilistic components while keeping authority. Like that is the definition of knowledge work now. Programming just ran into this first. So let's take a minute. Let's actually lay out what we mean by this new skill tree and why I think it's important for leaders to pay attention to it. I'm going to describe this skill tree as a hierarchy of nodes.
 Every node is a capability that you can demonstrate. Every node has a failure mode if you skip it. And the whole tree is built around this core idea that probabilistic models require you to separate your decisions from the act of token generation. And that's the root node we'll start with. If you don't understand that you have to separate generation from decisioning, everything else gets really chaotic because a probabilistic model is incredibly good at generating. It can generate drafts or options or code or summaries or transformations or hypotheses or structured outputs. What it is not allowed to do, if you want reliability, is to be the final authority. The workflow must decide, the system must decide, or the human must decide. But the model should not decide what's true, what's safe, and what's kind. I'm going to say that again because people might scratch their heads. When I say the workflow must decide or the system must decide, I mean you can architect models inside workflows in such a way that they produce extremely dependable, extremely accurate outputs measured against definitions of correctness that humans hold. And humans can then take edge cases. When I say the model should not decide, I mean that the LLM by itself without that workflow harness around it can't reliably decide what is correct. It can't reliably decide what's safe. It can't reliably decide what is approved or what should ship. When we get burned in the workplace with LLMs, it's almost always because we left a token generator to be the judge because I guess we got fooled by the hype. So the entire tree that I'm talking about here is really a set of skills that are required to do one thing. Let the model generate quickly while preserving human authority through the workflow. If you follow that through, I think the hierarchy is actually pretty intuitive. Level one is really about conditioning. And again, keep in mind as leaders, this is a way you can start to think about all of the different roles in your company that have to do with knowledge work. It is not just for engineers. So conditioning, steering a probabilistic component. The first node there might be intense specification. In a deterministic system, ambiguous requirements are still going to cause you problems, but the system won't hallucinate what you meant. In a probabilistic system like we have today, ambiguity is gasoline on the fire. The model will happily fill the gap with plausible nonsense. So you need a very, very tight problem contract and we just haven't been used to that in knowledge work. You need a very tight purpose, a tight audience, tight constraints, definitions, etc. This is not just managerial overhead. In the new world, it's steering the inputs so that you can reduce variance and increase the reliability of the outputs. Node number two is around context engineering. A huge amount of model failure is simply context failure. Wrong material, missing material, too much material, poor ordering of material, conflicting instructions, truncated history. Context engineering means you can reliably decide what goes into the context window, what stays out of the context window, what is summarized, what is quoted, what must be preserved verbatim, what's not trusted. This is the new IO and databases of the AI stack. And then node number three in this level is constraint design. So constraints are how you turn a token generator into a component that's reliable. You have defined output formats, defined schemas, defined rubrics. You have required citations, you have allowed tools, you have token budgets, stop conditions. A probabilistic systems without constraints is a slot machine. A probabilistic system with constraints becomes a reliable machine that can do work. So those are the first three pieces I would put into this sort of level one of the skill tree around conditioning, around steering, right? Can you steer with intent? Can you steer with context engineering? Can you steer with good constraints? Level two, once you have that, is really around keeping ownership without full authorship. It's what I would call authority in the age of AI. This is a difficult layer. It's the difference between I used AI and I know how to operate an AI system responsibly. The first thing to learn here is around verification design. How does truth come into the loop, right? How do you know what's correct? Because the model can generate a lot of plausible falsehoods and you need really, really explicit verification mechanisms. Some verification can be deterministic. Is it valid with the schema or not, right? Is it passing a unit test or not? Some verification is very procedural. So a human can review, can provide a second pass critique, can do adversarial prompting, et cetera. The key is that verification is not optional. It is the mechanism that replaces the old guarantee that you got from the authored logic that an engineer would develop. And so what you need to learn at this level is how can you design verifications that ensure tight alignment between system performance and correctness. Next, provenance and chain of custody becomes a factor here. Remember in the past, chain of custody was implicit, right? You can see the workflow and you know who wrote it, so you have chain of custody. But if authority requires provenance, how do we get that in the age of AI? If the output makes claims, you're going to need to be able to design a system that shows where those claims came from, sources, citations, quotes, retrieved documents. That is all part of designing a good probabilistic system today. In the deterministic world, you could get that straight out of the code with some testing. In the probabilistic world, it's about evidence. It's about establishing systems that author traceability as a first class object. You design them to be audited from day one. And then the next piece is permissions. The model cannot be your security boundary. That's a disaster. If the system is allowed to email customers, to move money, to change permissions, to merge code, you have to treat it like you treat permissioning in any other part of your system. The permissioning should be deterministic. It should be on the least privileged basis. And you should go through all the usual tools you have, allow lists, scope tools, approval steps, audit trails, et cetera. And this is where you actually can instantiate agents in a way that's useful, right? So if we look at these three, what I'm trying to get you to is an understanding of the elements of authority in the age of probabilistic machines. You have to be able to verify work. You have to be able to show provenance and chain of custody to have useful work. And you need permission envelopes so that the agents that you set out, you can prove that they are not over-permissioned. And that is going to be more and more of a security issue in 2026. So let's say you've learned about authority. You've learned about conditioning and intent steering. What's next on the skill tree here? Level three of the skill tree is really around workflows. How can you take intelligence as a raw material and turn it into a scaled-out factory? This is where the compounding leverage comes into play. So one piece here is learning how to decompose into pipeline steps. This is where you stop treating the model like a chatbot and you start treating it as a piece in a pipeline. You build intermediate artifacts. You create checkpoints. You keep the generator away from the final decision. You make failures local instead of global. And you make the workflow runnable by someone else, not just you. And then that goes with failure mode taxonomy. In deterministic systems, debugging is tracing logic. You can just trace it through the code, right? In probabilistic systems, debugging is really classifying failure modes and finding useful ways to address them. Was the context missing? What should we do about it? Was retrieval wrong? How do we adjust the context window? Did the tool fail? Did we declare the tool correctly? Did constraints conflict? Did it hallucinate? Was the task underspecified? Did the model just refuse to do it? Did it exceed budget? You basically need a complete taxonomy of errors so that you stop assuming that you can fiddle with the prompt every time and you start identifying the correct layer where the failure occurred and fixing it properly. And then the next piece here, all still inside workflows, is how you handle observability. How do you make the system legible? You cannot fully inspect the model's internal reasoning, so you have to compensate by making the surrounding system extremely observable. Traces of your tool calls, inputs used, documents retrieved, intermediate outputs, validations passed or failed, the timing, the cost. This is how you ensure that the system is legible all the way through your workflow. In a sense, you're taking the skills you learned about auditability at level 2 and you're scaling them to the workflow layer. And so the three pieces of the workflow layer together give you the room to extend your leverage. It's not just you doing things, you're now building automated systems. You can decompose into steps, you can diagnose failure modes, you can figure out observability on a complex system. You're starting to be able to scale LLMs. The final level is compounding. This is where the leverage becomes more durable instead of something you just set up and sort of go with once. Evaluation harnesses are so critical here. Without evals, it's difficult to compound. You just end up improvising faster. Look, evals can be small, they can be a golden set of examples, they can be regression tests for outputs, they can be scorecards or thresholds, but you do need a harness so that you can change your prompts, your models, and your retrieval methods or your tools without playing Russian roulette. You also need better feedback loops so that the system corrects itself. The highest leverage comes from your agent operating effectively in a loop where it can draft, critique, revise, recheck, and ship. Or it can retrieve an answer, it can cite, it can verify, and it can finalize. The loop makes the generator less risky because errors are caught within the system before final shipment.
 Prompts, your models, and your retrieval methods or your tools without playing Russian roulette. You also need better feedback loops so that the system corrects itself. The highest leverage comes from your agent operating effectively in a loop where it can draft, critique, revise, recheck, and ship. Or it can retrieve an answer, it can cite, it can verify, and it can finalize. The loop makes the generator less risky because errors are caught within the system before final shipment. At the same time, however, you just need to be able to build a really, really good evaluation loop. The last thing to keep in mind if you really want to scale this is learning the skill of drift management and governance. Models are going to change. Data is going to change. Teams are going to change. Attackers are going to adapt. Governance means versioning, etc. Auditability, policies, and so on. You need to start treating the work you do like production infrastructure, even if you're not used to thinking that way because you're not a technical person. And this is the final layer of authority, right? It's the ability to operate under a condition of continuous change without losing control over the system. So there you go. That's the compounding piece. Those are the four levels I am just sketching in that sort of the loose, high-level skill tree. There's a ton of work to be done to fill that out and actually get to a detailed curriculum, a detailed rubric that suits particular job families in the new AI era. But my goal here has been to share with you how you can take a reflection like Andres and say, yeah, you're right. There's a whole new set of skills to learn and start to think about it strategically, not just from an engineering perspective. So this is not really... I want you to notice, this is not really about learning AI tools. It's learning how to operate probabilistic systems as a compute service across your entire business. It applies to everybody. The lawyer building a contract review workflow and the engineer building a debugging agent are climbing the same skill tree today. They may have different artifacts, but they have the same hierarchy of skills. And this is where a very famous computer game becomes the perfect analogy. Factorio is a game about climbing exactly this kind of skill tree. If you've never heard of it, I'll give it to you quickly. It's just a video game where you land on a new planet and your job is to build an automated factory. So you start by handcrafting really basic items, but the system quickly pushes you into automation. And so you start to improve your mining. You start to install conveyor belts. You start to route more of your outputs into more factories. You eventually start to automate more and more of the supply chain. This is a great training metaphor for this era because it teaches us the instincts that actually scale. Decomposing problems scales. Modularity scales. Observability scales. Understanding where bottlenecks live in a system, that scales. Last radius estimation, that scales. We don't have to be attached to the quality of manual authorship to find meaning in our work. Nobody cares if you personally crafted a gear that goes into the machine. The thing that matters is that the system produces gears at scale that do useful work. We spent decades equating competence with authorship, especially in the engineering world. We celebrated super engineers that were good at this. But the world is now going to reward something else. It's going to reward anyone's ability, not just engineers, anyone's ability to design workflows that produce reliable outcomes. Even when the LLM token generator at the heart of the system is stochastic, is probabilistic, is partially opaque. That's not less skill. It's just different skill. And it's genuinely difficult because it forces you to replace the comfort of control with the discipline of systems. So if you feel behind, it's not that you're failing. It means you're correctly perceiving that the stack is different now. The way forward is not going to be frantic tool chasing. It's obviously not going to be denial. It's choosing to understand that we have a different skill tree, that all of us in the knowledge work world are climbing that tree together, and that we do a better job of that when we climb it deliberately. When we intentionally separate generation from decisioning. When we intentionally learn to condition the behavior of the system with artifacts and constraints. When we learn how to preserve authority in the system. When we learn how to build workflows, not just prompts. And when we can actually make systems compound with evals, with good feedback loops, etc. The new hierarchy won't be based on who codes the fastest. It will be based on who can orchestrate uncertainty without losing authority. And that's, I think that's what technical means now. It's for everyone. And yes, it's absolutely hard because we're learning to operate a new kind of machine while it's being invented. But for the organizations that recognize this as a challenge, these are the kinds of human skills that we all need to grow in in order to move faster in the AI era. This is the opportunity in front of us. The organizations that figure out how to take this understanding, appropriately detail it for their particular context, scale that across their workforce, those are the ones that are going to realize 10x speedups. The organizations that insist on the old hierarchies of technical versus non-technical, and I have my job hat and it's only product manager and I only do product management stuff, those are the ones that are not going to do well. So the choice is yours, but I think that this is the end of the technical versus non-technical era. And we need to start a skill tree for a new era. Best of luck.

```

</details>

---

*Generated with YouTube Summarizer using GPT-5.2 & gpt-4o-transcribe*
